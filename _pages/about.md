---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I received my Ph.D. from [Wuhan University of Technology](http://english.whut.edu.cn/) in 2024, and I was extremely fortunate to be supervised by [Prof. Luo Zhong](https://baike.baidu.com/item/%E9%92%9F%E7%8F%9E/9264964), one of the most dedicated teachers I have ever met.
My PhD research interests lie in action recognition under complex environments, video content analysis, and computing science. My ultimate aim is to use machine intelligence to help understand human action, facilitating daily life and health. 

I am currently delving into spike vision at Peking University, under the supervision of [Prof. Tiejun Huang](https://cs.pku.edu.cn/info/1215/1980.htm), a leading expert in [spike vision](https://spikecv.github.io/zh/index.html).

# 🤜🤛 Group 
I maintain close collaborations with [Prof. Zhong Xian](http://cst.whut.edu.cn/xygk/szdw/201505/t20150527_876884.shtml)'s team [XIAN Group](https://xiangroup.github.io/) at Wuhan University of Technology and [AIM Lab](http://aim-nercms.whu.edu.cn/) of [Prof. Wang Zheng](https://wangzwhu.github.io/home/) from Wuhan University.
If you are seeking any form of academic cooperation, please feel free to email at liuwx66@pku.edu.cn.

# 🔥 Action-related News
- *2024.12*: &nbsp;🎉🎉 One paper was accepted by Journal of Image and Graphics, 中国图象图形学报. 
- *2024.08*: &nbsp;🎉🎉 One paper was accepted by PR.
- *2024.07*: &nbsp;🎉🎉 One paper was accepted by ACM MM 2024 International Workshop on Human-centric Multimedia Analysis.
- *2024.05*: &nbsp;🎉🎉 One paper was accepted by The Visual Computer.
- *2024.07*: &nbsp;🎉🎉 One co-authored paper (时空语义驱动的渐进多视角行为去偏置研究) was accepted by Computer Engineering, 计算机工程.
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024.
- *2024.03*: &nbsp;🎉🎉 One co-authored paper was accepted by CVPR 2024.
- *2023.10*: &nbsp;🎉🎉 One co-authored paper was accepted by ICIP.
- *2023.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2023.
- *2023.06*: &nbsp;🎉🎉 Two co-authored papers were accepted by ICASSP 2023.
- *2023.05*: &nbsp;🎉🎉 One paper was accepted by TIP.
- *2022.12*: &nbsp;🎉🎉 One paper was accepted by SPL.
- *2022.06*: &nbsp;🎉🎉 One co-authored paper was accepted by ICASSP 2022.
- *2021.11*: &nbsp;🎉🎉 Two co-authored papers were accepted by PRICAI 2021.
  
# 🔥 Spike-related News
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024.
- *2024.04*: &nbsp;🎉🎉 One co-authored paper was accepted by TCDS.

  
# 🔥 Other News
- *2024.12*: &nbsp;🎉🎉 One co-authored paper was accepted by CVIU (CCF B). Congradulations to YES!
- *2024.10*: &nbsp;🎉🎉 One co-authored paper was accepted by AAAI2025.
- *2024.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2024 Workshop on Multi-modal Misinformation Governance in the Era of Foundation Models.
- *2023.11*：&nbsp;🎉🎉 One co-authored paper was accepted by PR.
- *2023.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2023.
- *2023.06*: &nbsp;🎉🎉 Two co-authored papers were accepted by ICASSP 2023.
- *2022.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ICME 2022.
- *2022.06*: &nbsp;🎉🎉 One co-authored paper was accepted by TIP.
- *2021.11*: &nbsp;🎉🎉 One co-authored paper was accepted by MMM.
- *2021.07*: &nbsp;🎉🎉 One co-authored paper was accepted by ACM MM 2021.
- *2020.06*: &nbsp;🎉🎉 One co-authored paper was accepted by ICMR 2020.

# 📝 Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PR</div><img src='_pages/DSMF0422_1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dynamic and static mutual fitting for action recognition](https://www.sciencedirect.com/science/article/abs/pii/S003132032400699X)

**Wenxuan Liu**, Xuemei Jia, Xian Zhong, Kui Jiang, Xiaohan Yu, Mang Ye

[**PDF**](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320324X00096/1-s2.0-S003132032400699X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENH%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIF1U5uXWS%2FOm3JXuwaGPChBKbU7e4ToGrr%2BHkOQ%2F2zpTAiEAg%2BTiI1MiNeJVaCC0xzWRWbJwbJqlOMcO%2FcW6d9KQQl4quwUIif%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDLoAVzem1PRBfuFNVyqPBfE9n8dKH4XrFE%2Bb8IELBSNpTYWrYxiJaRvjVK4kBjriN1udR9NQW2bUlqWY0x%2BhO34XzPj6cf3FAYwJ4QbJB3Fex62cBpH530Xanc1AzOoalrD7CZEpGdEpBkpuLTYyuH%2FZJbpD%2F%2FcTrrnejkeLTV7ryzNRKpAuL%2FQnE%2FsuXYynv4cesny66Gq7j9va2z5GHY6CG6P4EKMeDVlZ1eAkhzzhaZZLFKaqnDpbGW3LQAUEGv0xzPm0bMiF0MMK1dblialTey7C69MwBG9SyTnGAACLnnd8fp2kw%2FuFwgkilknD3Om%2FMejWlTvoYstkQkVY4cxKbtsu48jpxwBC1TbncoVzaiPsvCt7xu0xwCJVUshXoJKjaSz5fQwT1Wjys%2FoKhWlNwq0JdqKoWEXTMkge0UBLrZLu8MD%2B0tJEcmakQhUhF%2FJqssIOFxKSMuvndT5PIpxXYJslRJGdloP%2BoV8mjINMMcXqiAqBWWnWJokrImfsyoxI7VlDqIqPwXdxpN8RsrlxODa5uahY3wTJkt4WP6i4tUMcYEvJ7Z8AYBsSH62AfM3PLkHJHXfC54iu4g5AG1U6C4YnJZ6bblK%2BYT5%2Bkv9%2BD%2FrkbucoZWvey9M22OXoGZPG0DiS2%2Fw7XpA%2BfOG4GIS8yZiQLLOtXWnD2prYrgfDGqpEmwtknwEFkKtAKOCetWkbV6OtqFyFqnWuaLImoFXkw63WOdfgyJNJPFDzVSmnIi%2BtYBqIEs8LQpv8MsXdIRiK%2FXmF0Yu%2FRXdTuRetcoU9x66RWOiS1dKZfVDqRZBYNzi6TIx2Fx0QAgKQ7yrNdQn%2BAhtLeEe1IY%2FjpNsQimJ311z4vppULbTMA0KeeG7C%2BAJAU8LSFw%2FgugeH9ugwj%2FLfugY6sQF2rXMdHBIRYBe1qoeeGgzqoXqoAHEFYgUbW%2F87qJlDBEl4yr4MeQgZlC%2Feicd3XIUHYgBMM87Hu6DliN%2FXGkPZWqqD8B9TCHE9h6r8EGYhgmeLinJp2hAeNGnUNr0IqyADn4x5nNbXu3Tw%2BuwlWVhzdMSDC%2FuqW1vT8LkuTdy9H1avMO%2BM6Co%2BJty5NKropLPikM4e3GFXQlIt4Il266bCEYNoEmW3itXtk8NpOaAL3E4%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241210T093545Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUAQ5FZ5J%2F20241210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b841bd330c2f20490b381c1c57b56212329f65393ecaa669dc67d90781732bf2&hash=ecb7ee46ad52995363054f5728a92424266d5c804c4f8430c42cccd563f95b59&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S003132032400699X&tid=spdf-aae64fbc-1e79-443c-97f6-56625a275d31&sid=c15ee3ac8f5c964ea968d122cd8d6c6150a4gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=190b5f0a070357035c5656&rr=8efc3f267f40ddc0&cc=cn) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- We propose Dynamic Temporal-aware Erasing (DTE) to excavate the fine-grained representation based on actor trajectory without destroying temporal information. 
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP</div><img src='images/1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dual-Recommendation Disentanglement Network for View Fuzz in Action Recognition](https://ieeexplore.ieee.org/abstract/document/10122859)

**Wenxuan Liu**, Xian Zhong, Zhuo Zhou, Kui Jiang, Zheng Wang, and Chia-Wen Lin

[**PDF**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10122859) [**CODE**](https://github.com/51cloud/DRDN)<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- This paper perceives a ubiquitous view fuzz issue in multi-view action recognition tasks, which undermines the action feature distillation by hiding essential action regions.
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM HCMA</div><img src='_pages/0809_1.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Pixel-REfocused Navigated Tri-margin for Semi-supervised Action Detection](https://dl.acm.org/doi/abs/10.1145/3688865.3689478)

**Wenxuan Liu**, Shilei Zhao, Xiyu Han, Aoyu Yi, Kui Jiang, Zheng Wang, Xian Zhong (Best Student Runner-up)

[**PDF**](https://dl.acm.org/doi/pdf/10.1145/3688865.3689478) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- This paper identifies a novel issue, termed pixel activation-uncertainty, in semi-supervised action detection, which highlights the difficulty in distinguishing between action and background boundaries due to active motion. 
</div>
</div>

- [YES: You should Examine Suspect Cues for Low-Light Object Detection]{https://www.sciencedirect.com/science/article/abs/pii/S1077314224003527?casa_token=bnigivn1YoMAAAAA:3vMY9IlCJtY42ys36mh9qFoDE-jAJtjF16qR4dsKBLp8xqGxq-O2OW40_dfnxZN0kS1EKGN7660}, Shu Ye, Wenxin Huang, **Wenxuan Liu**, Liang Chen, Xiao Wang, Xian Zhong, **CVIU**
- [Pioneering Explainable Video Fact-Checking with a New Dataset and Multi-role Multimodal Model Approach](https://github.com), Kaipeng Niu, Danni xu, Bingjian Yang, **Wenxuan Liu**, Zheng Wang, **AAAI 2025**
- [Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks](https://github.com/hsw0929/HSD), Xian Zhong, Shengwang Hu, **Wenxuan Liu***, Wenxin Huang, Jianhao Ding, Zhaofei Yu, Tiejun Huang, **ACM MM 2024**
- [Predicting the Unseen: A Novel Dataset for Hidden Intention Localization in Pre-abnormal Analysis](https://github.com/Zzz99999/Hidden_Abnormal_Intention), Zehao Qi, Ruixu Zhang, Xinyi Hu, **Wenxuan Liu**, Zheng Wang, **ACM MM 2024**
- [Assess and Guide: Multi-modal Fake News Detection via Decision Uncertainty](https://dl.acm.org/doi/abs/10.1145/3689090.3689389), Jie Wu, Danni Xu, **Wenxuan Liu**, Joey Zhou, Yew Ong, Siyuan Hu, Hongyuan Zhu, Zheng Wang, **ACM MM 2024 Workshop Multi-modal Misinformation**
- [Bi-Causal: Group Activity Recognition via Bidirectional Causality](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Bi-Causal_Group_Activity_Recognition_via_Bidirectional_Causality_CVPR_2024_paper.pdf), Youliang Zhang, **Wenxuan Liu**, Danni Xu, Zhuo Zhou, Zheng Wang, **CVPR 2024**
- [ICLR: Instance Credibility-Based Label Refinement for label noisy person re-identification](https://www.sciencedirect.com/science/article/abs/pii/S0031320323008658), Xian Zhong, Xiyu Han, Xuemei Jia, Wenxin Huang, **Wenxuan Liu**, Shuaipeng Su, Xiaohan Yu, Mang Ye, **PR 2024**
- [Converting Artificial Neural Networks to Ultra-Low-Latency Spiking Neural Networks for Action Recognition](https://ieeexplore.ieee.org/abstract/document/10466357), Hong You, Xian Zhong, **Wenxuan Liu**, Qi Wei, Wenxin Huang, Zhaofei Yu, Tiejun Huang, **TCDS 2024**
- [Uncovering the Unseen: Discover Hidden Intentions by Micro-Behavior Graph Reasoning](https://dl.acm.org/doi/abs/10.1145/3581783.3611892), Zhuo Zhou, **Wenxuan Liu***, Danni Xu, Zheng Wang, Jian Zhao, **ACM MM 2023**
- [Dawn: Direction-aware attention wavelet network for image deraining](https://dl.acm.org/doi/abs/10.1145/3581783.3611697), Kui Jiang, **Wenxuan Liu**, Zheng Wang, Xian Zhong, Junjun Jiang, Chia-Wen Lin, **ACM MM 2023**
- [Background-weakening consistency regularization for semi-supervised video action detection](https://ieeexplore.ieee.org/abstract/document/10095478), Xian Zhong, Aoyu Yi, **Wenxuan Liu***, Wenxin Huang, Chengming Zou, Zheng Wang, **ICASSP 2023**
- [Neighborhood information-based label refinement for person re-identification with label noise](https://ieeexplore.ieee.org/abstract/document/10096080), Xian Zhong, Shuaipeng Su, **Wenxuan Liu***, Xuemei Jia, Wenxin Huang, Mengdie Wang, **ICASSP 2023**
- [Implicit Attention-Based Cross-Modal Collaborative Learning for Action Recognition](), Jianghao Zhang, Xian Zhong, **Wenxuan Liu***, Kui Jiang, Zhengwei Yang, Zheng Wang, **ICIP 2023**





# 🎖 Honors and Awards
- *2024.10* ACM MM 2024 the 5-th International Workshop on Human-centric Multimedia Analysis, Best Student Paper Runner-Up.
- *2024.07* Outstanding Graduate Student of Wuhan University of Technology.
- *2023.07* First Prize in the ICME ’23 Grand Challenge “Seeing Through the Rain (STRAIN): Vision Task Challenges in Real-world Rain Scenes”, Track 3.
- *2022.08* Third Prize in the 11th China Software Cup Student Software Design Competition

# 📖 Educations
- *2024.06 - now*, Postdoc, Peking University, China.
- *2019.09 - 2024.06*, Ph D, Wuhan University of Technology, China.
- *2017.09 - 2019.06*, Master, Wuhan University of Technology, China.
- *2012.09 - 2016.06*, Bachelor, Xi’an Polytechnic of University, China. 

